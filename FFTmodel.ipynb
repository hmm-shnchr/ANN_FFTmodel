{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from make_dataset import make_dataset\n",
    "from artificial_neural_network import ArtificialNeuralNetwork\n",
    "#from make_train_test_dataset import MakeTrainTestDataset\n",
    "#from reshape_merger_tree import ReshapeMergerTree\n",
    "import numpy as np\n",
    "import copy as cp\n",
    "import os, sys\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Parameters--\n",
      "1. param_1e+7_1e+18_ScaleFactor_ID_pid_Mvir_Rvir_x_vx_MW039_MW038_MW035_MW034_MW033_MW032.pickle\n"
     ]
    }
   ],
   "source": [
    "##Print a dir_list which is a list of ectracted parameters.\n",
    "##The parameters are made by ./parameter/MakePickleOfParam.ipynb.\n",
    "dir_list = np.loadtxt(\"param_list.txt\", dtype = \"str\")\n",
    "for i in range(len(dir_list)):\n",
    "    if i == 0:\n",
    "        print(dir_list[i])\n",
    "    else:\n",
    "        print(\"{}. {}\".format(i, dir_list[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Select index>> 1\n"
     ]
    }
   ],
   "source": [
    "##Input index-number of param_list.txt in dir_num to select it.\n",
    "##Load the parameters of host-halo and sub-halo.\n",
    "dir_num = int(input(\"\\nSelect index>> \"))\n",
    "with open(\"parameters/\" + dir_list[dir_num], mode = \"rb\") as f:\n",
    "    param = pickle.load(f)\n",
    "with open(\"parameters/host_\" + dir_list[dir_num], mode = \"rb\") as f:\n",
    "    host_param = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters : ['ScaleFactor', 'ID', 'pid', 'Mvir', 'Rvir', 'x', 'vx']\n",
      "files : ['mainbranch_MW039.csv', 'mainbranch_MW038.csv', 'mainbranch_MW035.csv', 'mainbranch_MW034.csv', 'mainbranch_MW033.csv', 'mainbranch_MW032.csv']\n"
     ]
    }
   ],
   "source": [
    "##The list of used parameters and host-halo.\n",
    "param_list = list(param.keys())\n",
    "mainbranch_list = list(param[param_list[0]].keys())\n",
    "print(\"parameters : {}\".format(param_list))\n",
    "print(\"files : {}\".format(mainbranch_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Get zacc of each sub-halo as acc_sf(Data type is dictionary).\n",
    "##The zacc is a time when ID of host-halo and pid of sub-halo match.\n",
    "acc_sf = {}\n",
    "for m_key in mainbranch_list:\n",
    "    acc_sf[m_key] = []\n",
    "    for idx, parameter in enumerate(param[\"pid\"][m_key]):\n",
    "        for i in range(parameter.size):\n",
    "            host_i = host_param[\"ID\"][m_key].size - parameter.size + i\n",
    "            if host_param[\"ID\"][m_key][host_i] == parameter[i]:\n",
    "                acc_sf[m_key].append(i)\n",
    "                break\n",
    "            elif i == parameter.size-1:\n",
    "                acc_sf[m_key].append(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Describe variables of learning-parameter of a DeepLearningModel in LearnParam.\n",
    "class LearnParam:\n",
    "    def __init__(self):\n",
    "        self.eps = 1e-7  ##Prevent devision by zero like 1 / (0 + eps).\n",
    "        self.bias = False ##If This bias is True, add normalized dataset to bias.\n",
    "        self.hidden = [100]*10  ##Defines number of hidden layers and neurons in each layers.\n",
    "        self.batchsize_denominator = 100  ##Defines a size of mini-batch as size(train-dataset) /  batch_denominator.\n",
    "        self.learning_rate = \"1e-3\"  ##Learning rate of Back propagation.\n",
    "        self.optimizer = \"Adam\"  ##Optimizer of the DeepLearningModel.\n",
    "        self.batch_normalization = True  ##Whether to use BatchNormalization or not.\n",
    "        self.loss_func = \"MSE_RE\"  ##Loss function of a DeepLearning model.\n",
    "        self.activation_func = \"tanhexp\"  ##Activation function of the hidden layers.\n",
    "        self.weight_init = \"he\"  ##Condition for initializing weight of the hidden layers.\n",
    "        self.lastlayer_identity = True\n",
    "        self.epoch = 200  ##Training epoch.\n",
    "        self.input_size = 3  ##Size of input-dataset(axis == 1)\n",
    "        self.output_size = 7  ##Size of output-dataset(axis == 1)\n",
    "        self.param_kind = \"x\"  ##Learning parameter.\n",
    "        self.train_ratio = 0.9  ##Percentage of training-dataset to all-dataset.\n",
    "        self.fft_format = \"rfft\"\n",
    "        self.normalize_format = \"Standardization\"  ##Selects a format of dataset normalization in None, Normalization, Standardization.\n",
    "        self.extract_dataset = \"After_acc\"  ##Selects a condition of dataset.\n",
    "        ##All is overall, Before/After_acc is in part of before/after accretion, All(_not)_acc is overall of (not)accreted sub-halo.\n",
    "        self.learn_num = 1  ##Distinguishes each directories where training results are saved.\n",
    "        self.save_fig_type = \".png\"  ##Format of the saved figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "LP = LearnParam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = make_dataset(mainbranch_list, LP.param_kind, host_param, param, LP.extract_dataset, acc_sf, LP.input_size, LP.output_size)\n",
    "#MTTD = MakeTrainTestDataset(mainbranch_list)\n",
    "#train, test = MTTD.split(cp.deepcopy(dataset), LP.train_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANN = ArtificialNeuralNetwork(LP.input_size, LP.hidden, LP.activation_func, LP.weight_init, LP.batch_normalization, LP.output_size, LP.lastlayer_identity, LP.loss_func, is_epoch_in_each_mlist = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make a train/test dataset.\n",
      "Mini-batch size : 63\n",
      "Iterations per 1epoch : 100\n",
      "Total iterations : 20000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-13ca6e7ce278>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mANN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_ratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatchsize_denominator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfft_format\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/master/ANN_FFTmodel/artificial_neural_network.py\u001b[0m in \u001b[0;36mlearning\u001b[0;34m(self, dataset, train_ratio, opt, lr, batchsize_denominator, epoch, fft_format, norm_format)\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mbatch_output_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_output_imag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcupy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_output_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcupy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_output_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0mgrads_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork_real\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_input_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_output_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m             \u001b[0mgrads_imag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork_imag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_input_imag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_output_imag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m             \u001b[0mparams_network_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork_real\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0mparams_network_imag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork_imag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/master/ANN_FFTmodel/multilayer_extend_gpu.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, x, t, is_training)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mdout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0;31m##Update gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/master/ANN_FFTmodel/layers_gpu.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, dout)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mdx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/cupy/_math/sumprod.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# TODO(okuta): check type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ANN.learning(dataset, LP.train_ratio, LP.optimizer, LP.learning_rate, LP.batchsize_denominator, LP.epoch, LP.fft_format, LP.normalize_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_input, data_output = None, None\n",
    "RMT = {}\n",
    "for m_key in mainbranch_list:\n",
    "    RMT[m_key] = ReshapeMergerTree()\n",
    "    input_, output_ = RMT[m_key].make_dataset(dataset[m_key], LP.input_size, LP.output_size)\n",
    "    if data_input is None and data_output is None:\n",
    "        data_input = input_\n",
    "        data_output = output_\n",
    "    else:\n",
    "        data_input = np.concatenate([data_input, input_], axis = 0)\n",
    "        data_output = np.concatenate([data_output, output_], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_unit = np.concatenate([data_input[:, :LP.input_size], data_output, data_input[:, LP.input_size:]], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fft_input = np.fft.fft(data_input)\n",
    "#ifft_input = np.fft.ifft(fft_input)\n",
    "fft_input = np.fft.rfft(data_input)\n",
    "ifft_input = np.fft.irfft(fft_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.96984659e-15 -1.17961196e-15  0.00000000e+00  3.55271368e-15\n",
      "   3.55271368e-15  7.10542736e-15]\n",
      " [ 5.96744876e-16 -4.13558077e-15  0.00000000e+00  3.55271368e-15\n",
      "   3.55271368e-15  3.55271368e-15]\n",
      " [-1.77635684e-15  0.00000000e+00 -5.32907052e-15  0.00000000e+00\n",
      "   0.00000000e+00  3.55271368e-15]]\n"
     ]
    }
   ],
   "source": [
    "print(ifft_input[:3] - data_input[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fft_unit = np.fft.fft(data_unit)\n",
    "#ifft_unit = np.fft.ifft(fft_unit)\n",
    "fft_unit = np.fft.rfft(data_unit)\n",
    "ifft_unit = np.fft.irfft(fft_unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7.63278329e-16  3.55271368e-15  1.77635684e-15  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00 -3.55271368e-15  0.00000000e+00\n",
      "  -3.55271368e-15 -3.55271368e-15  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00 -3.55271368e-15]\n",
      " [-2.49800181e-16 -2.78943535e-15  1.77635684e-15 -3.55271368e-15\n",
      "   3.55271368e-15 -3.55271368e-15  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  3.55271368e-15 -3.55271368e-15\n",
      "  -3.55271368e-15 -3.55271368e-15]\n",
      " [ 1.77635684e-15  1.77635684e-15 -3.55271368e-15  1.77635684e-15\n",
      "   3.55271368e-15 -3.55271368e-15  0.00000000e+00  0.00000000e+00\n",
      "   3.55271368e-15  7.10542736e-15  0.00000000e+00  3.55271368e-15\n",
      "  -3.55271368e-15  3.55271368e-15]]\n"
     ]
    }
   ],
   "source": [
    "print(ifft_unit[:3] - data_unit[:3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
